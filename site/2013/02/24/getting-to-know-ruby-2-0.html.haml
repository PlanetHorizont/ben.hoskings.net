---
layout: "/_post.haml"
title: "getting to know ruby 2.0"
css_class: "with-margin"
---

:md
  Today is an auspicious day for ruby. Exactly 20 years ago today, [Matz](https://twitter.com/yukihiro_matz) started work on what was to become ruby. It saw its first public release about three years later, at the end of 1995. Since then many rubies have been cut and polished. So happy 20th birthday to ruby, and a tip of the hat to Matz and all on ruby core who've worked hard on it since then.

  Hopefully well see another big event today---the release of ruby-2.0. This is a release that's been years in the making. Version 2.0 has a strong theme, and that's _convention_---2.0 formalises useful conventions that us rubyists have grown to use, and it judiciously adds language features to address bad conventions.

  I spoke at [RubyConf AU](http://www.rubyconf.org.au) on Friday about these changes. Firstly, here's a run-down of some more specific changes. There's a full list [in the NEWS file](https://github.com/ruby/ruby/blob/trunk/NEWS), but these are my favourites:

  - **The GC is copy-on-write friendly**, thanks to [Narihiro Nakamura](https://twitter.com/nari_en). In the past, forked ruby processes would quickly duplicate their shared memory, because the GC used to modify every object during the mark phase of its mark-and-sweep run. As of 2.0, objects are marked in a separate data structure instead of on the objects themselves, leaving them unchanged and allowing the kernel to share lots of memory. In practice, this means your unicorns will consume less resident memory (although they'll still appear to have the same resident size [because of how RSIZE is reported](http://unix.stackexchange.com/a/34867)). Pat Shaughnessy wrote [an excellent post](http://patshaughnessy.net/2012/3/23/why-you-should-be-excited-about-garbage-collection-in-ruby-2-0) detailing how Narihiro's new GC design works.

  - **Syck has been removed**. The syck/psych yaml gauntlet that we all had to pass through around the 1.9.2 days is completely behind us now, which is great. Ruby now has a hard dependency on libyaml, but it's bundled for the cases where the library isn't present locally.

  - **The vendored rubygems has been upgraded** to 2.0.0-preview2, which should be 2.0.0 final by release time. This new rubygems has full support for the new default gems that some vendored libaries have been moved into. Other notable changes include `gem search` being a remote search by default, and only `ri` docs being built by default. Good changes all round.

  - **The default source encoding is UTF-8 now** (changed from US-ASCII). This means literal strings are unicode by default in 2.0, and the `# coding: utf-8` comment we've grown accustomed to adding isn't required anymore. This is a nice step in the direction of "all unicode all the time". A bunch of other encoding cleanups were made, too -- one example is `Time#to_s`, which returns a string encoded in US-ASCII instead of BINARY.

  - **There's a new `#to_h` convention** to mirror `#to_a`. There are less hash-like things than array-like things, so it's not defined in as many places, but one nice addition is `Struct#to_h`, which returns a hash of the struct's keys and values: `Struct.new(:key).new('value').to_h #=> {:key=>"value"}`

  - **A new `%i[]` / `%I[]` array literal was added**, which produces an array of symbols, just like `%w[]` & `%W[]` produce arrays of strings. I can't think of many situations where I'd want to use this syntax, but I think it makes sense to have it alongside its string equivalent.

  - **`Zlib` supports streaming** when deflating and inflating now, which means large files can be processed without using up huge amounts of memory. This is a particularly nice change because MRI doesn't ever reduce its memory footprint while running -- growing the ruby process to allocate large objects is a one-way thing, even after those objects have been garbage collected.

  - **`IO#lines` and friends have been deprecated.** The list-of-string methods on `IO`, `StringIO`, `Zlib::GzipReader` and so on, like `#lines`, `#chars` and `#bytes`, have been deprecated in favour of `#each_line`, etc. This is a nice change, pushing the feel of the enumerating API in the direction of the `#each` / `#each_thing` convention.


:md
  ## Keyword arguments

  Keyword args are the perfect example of convention in 2.0. They very neatly address the pattern of passing optional named args in a trailing hash.

  Some have criticised the feature for not being a true named argument implementation, like python has. That's true, but I don't think it's a fair criticism because they're not intended as such: they're quite focused on solving just the optional trailing argument problem.

:captionedruby
  def render(source, opts = {})
    opts = {fmt: 'html'}.merge(opts)
    r = Renderer.for(opts[:fmt])
    r.render(source)
  end
  render(template, fmt: 'json')

  This is the essence of the design we're used to: pass a hash as the final argument, merge it into the defaults, and then pull keys out of it as required.

:md
  In ruby 2.0, this convention has a language-level version:

:captionedruby
  def render(source, fmt: 'html')
    r = Renderer.for(fmt)
    r.render(source)
  end
  render(template, fmt: 'json')

  Beautiful!

:md
  A couple of things to notice here. Firstly, no default handling is required, because it's part of the definition. Secondly, both the defining and calling syntax use the 1.9 hash syntax, so keyword args already feel familiar.

  Here's a real-world example: the definition of `#accepts_nested_ from within actionpack.

:captionedruby
  def accepts_nested_attributes_for(*attr_names)
    options = {
      :allow_destroy => false,
      :update_only => false
    }
    options.update(attr_names.extract_options!)
    options.assert_valid_keys(
      :allow_destroy,
      :reject_if,
      :limit,
      :update_only
    )
    # ...
  end

  As I said on Friday, I need a cup of tea and a lie down by the time I get to the method body.

:md
  Notice that there are three jobs being done by that code: extraction from the splatted args, rejection of invalid arguments, and default handling. Here's the 2.0 equivalent:

:captionedruby
  def accepts_nested_attributes_for(*attr_names,
    allow_destroy: false,
    update_only: false
    reject_if: nil,
    limit: nil
  )
    # ...
  end

  I'll still take the tea, if it's going.

:md
  This is about as condensed as that information can get, and all three jobs the manual version was doing---splat extraction, unexpected args, and defaults---are taken care of. (Defaults are mandatory, and unexpected keys raise an `ArgumentError`.) All in all a great feature that I think will clean up a lot of codebases.



:md
  ## Refinements

  Matz announced a new feature for ruby 2.0 called classboxing, "non-global monkey patching" at RubyKaigi '10. Since then the idea has evolved and the result, refinements, are an experimental feature in 2.0. They're a way of patching a class only within a certain scope.

:captionedruby
  module Patches
    refine Array do
      def collapse pattern
        grep(pattern).
          map {|i| i.sub(pattern, '') }
      end
    end
  end

  using Patches

  puts `git branch`.split("\n").collapse(/\* /)

  This is a refinement. The patch is only active in a given lexical scope after the `using` call.

:md
  At present, you can only call `using` at the top level of a file, but in future it will most likely be callable within a class or module too.

  There's been [a lot of discussion](http://bugs.ruby-lang.org/issues/show/4085) about how refinements will work, most of it around the merits of _local rebinding_ vs. strict lexical scoping.

  Local rebinding means that a binding's active refinements would also be active in procs `instance_eval`ed against that binding, even if the proc wasn't defined in a scope where those refinements were active. Ultimately, the question is whether refinements should be scoped statically or dynamically. Which refinements should count---those active in the current runtime scope, those from the lexical scope where the code was defined?

  Dynamic scoping would mean code could be run against unexpected refinements after the fact, which is the exact problem with monkey patching that refinements aim to solve. Hence, I believe lexical scoping is the right choice.

  [Charles Nutter](http://twitter.com/headius) [argued strongly](http://bugs.ruby-lang.org/issues/4085#note-70) that lexical scoping is the right choice, and I agree. [Yehuda Katz](http://twitter.com/wycats) wrote [a great post](http://yehudakatz.com/2010/11/30/ruby-2-0-refinements-in-practice/) on the subject too.

  Lexical scoping does have a surprising implication, though, that it's important to be aware of.

:captionedruby
  class Dict
    def word_list
      File.open('/usr/share/dict/words')
    end

    def long_words
      word_list.sort_by(&:length).reverse
    end
  end

  puts Dict.new.long_words.take(10)

  The simple case: `#long_words` calls `#word_list` to obtain the full list of words to operate on.

:md
  Suppose that we refine this class to reimplement `#word_list`.

:captionedruby
  module NameDict
    refine Dict do
      def word_list
        File.open('/usr/share/dict/propernames')
      end
    end
  end

  using NameDict

  puts Dict.new.long_words.take(10)

  This will return the longest words from the original dictionary, not the refined one here. Huh?

:md
  This is the implication of strict lexical scoping: only refinements that were active at the static callpoint count. In this case, we're not calling `#word_list` in a refined scope.

  To see why, trace where the call to `Dict.new.long_words` goes. At the callpoint, the `NameDict` refinement is active. But the refined method, `#word_list`, isn't called in that scope: it's called by the `#long_words` method up there in the original class, and in that scope the refinement isn't active.

  This seems surprising and a bit limiting at first, but it actually makes a lot of sense, because what looks like a limitation here is actually the exact constraint we need in order to make monkey patching safe. Along with this constraint comes a powerful guarantee: code will always call the version of the method it was written to call. This is good isolation, and means that refinements won't cause collateral damage.



:md
  ## Lazy enumerations

  Declarative list programming is even nicer in 2.0 thanks to lazy lists. Expensive and even infinite lists are cheaply useable now, because their elements are only evaluated as they're requested.

:captionedruby
  def natural_numbers
    (1..Float::INFINITY).lazy
  end

  def primes
    natural_numbers.select {|n|
      (2..(n**0.5)).all? {|f|
        n % f > 0
      }
    }
  end

  puts primes.take(10).force

  Here we're filtering the natural numbers (an infinite list) to just those that are prime (another infinite but less dense list). The `#force` method is just an alias to `#to_a`.

:md
  This is great. Suddenly we can deal with unweildy lists the way ruby does best: by giving them clear names and chaining them meaningfully.

  Custom enumerators can be defined for other types of data, too. It's already possible to read a file line-by-line using `#gets`, but still, here's an example:

:captionedruby
  def lazy_lines(io)
    Enumerator::Lazy.new(io) do |enum, i|
      enum << i
    end
  end

  def words
    lazy_lines(File.open('/usr/share/dict/words'))
  end

  puts words.select {|l| l.length > 12 }.take(10).to_a

  This is an example of enumerating a custom type (in this case an IO), by defining a new lazy enumerator. This would be an expensive operation, but laziness means we can stop reading as soon as we have 10 words that are long enough.

:md
  ## And now, a bit more on refinements

  Here's a rewrite of our infinite list example, this time pushing the prime number logic into a refinement on Fixnum. Check the two select styles at the bottom for another lexical scoping surprise.

:captionedruby
  module Maths
    refine Fixnum do
      def prime?
        (2..(self**0.5)).all? {|f|
          self % f > 0
        }
      end
    end
  end

  def natural_numbers
    (1..Float::INFINITY).lazy
  end

  using Maths

  puts natural_numbers.select {|i| i.prime? }.take(10).force # :)
  puts natural_numbers.select(&:prime?).take(10).force       # &, you're breaking my heart.

  &nbsp;

:md
  In the second example, the `&` in `&:prime?` invokes `Symbol#to_proc` to define a block equivalent to the literal one in the first example. But that process happens away inside the Symbol class, where no refinement is active. That is, at the point those values actually receive the `#prime?` method, there's no refinement active.

  My guess is that this will be added as a special case, because at this point `&:method` is a language feature in its own right.

  Either way, the integration of refinements and the way they feel are definitely unfinished, but I think they're looking very promising.
